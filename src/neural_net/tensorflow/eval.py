'''
Script to evaluate the predictions in a prediction file generated by predict.py
Updated 1 December 2020
'''
import argparse
import os.path as path
import matplotlib.pyplot as plt
from sklearn.metrics import auc, brier_score_loss
import numpy as np
import sys

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--predict-file', type=str, required=True)
    args = parser.parse_args()

    # it is assumed that predict_file is in the format [model name]_pred.txt
    # and that this file is inside the model directory already
    # this script saves the output in a file named [model name]_eval.txt
    # inside the model directory
    
    with open(args.predict_file, 'r') as f:
        lines = f.readlines()
    lines = [l.strip().split('\t')[1:] for l in lines]
        
    # https://www.kdnuggets.com/2017/04/must-know-evaluate-binary-classifier.html
    total = len(lines)
    TP = 0 # True Positives
    FP = 0 # False Positives
    TN = 0 # True Negatives
    FN = 0 # False Negatives

    for line in lines:
        label = line[0]
        pred = line[1]
        if label == 'TRUE':
            if pred == 'TRUE':
                TP += 1
            if pred == 'FALSE':
                FN += 1
        if label == 'FALSE':
            if pred == 'TRUE':
                FP += 1
            if pred == 'FALSE':
                TN += 1
                
    # True Positive Rate (TPR) aka Recall aka Sensitivity = TP/(TP+FN)
    if TP+FN==0:
        TPR=1
    else:
        TPR = TP/(TP+FN)

    # False Positive Rate(FPR) aka False Alarm Rate = 1-Specificity
    if TN+FP==0:
        FPR=1
    else:
        FPR = FP / (TN + FP)

    # Accuracy
    accuracy = (TP + TN) / (TP + TN + FP + FN)

    # Error Rate = (FP + FN) / (TP + TN + FP + FN)
    error_rate = 1- accuracy

    # Precision = TP / (TP + FP)
    if TP+FP==0:
        precision=1
    else:
        precision = TP / (TP + FP)

    # F-score = 2 / ( (1 / Precision) + (1 / Recall) )
    if TP+FP+FN==0:
        F_score=1
    else:
        F_score = TP/(TP+0.5*(FP+FN))

    probs_file = args.predict_file.replace('_pred.txt', '_probs.txt')
    with open(probs_file, 'r') as f:
        str_probs = [l.rstrip().split() for l in f.readlines()]
        probs = np.array([[float(l[0]), float(l[1])] for l in str_probs])

    true_labels = np.array(lines)[:, 0] == 'TRUE'
    tpr = []
    fpr = []
    for thresh in np.linspace(0, 1, num=100):
        sys.stdout.write(
            f"\rROC: Working on threshold {thresh//0.01/100} / 1.00 "
        )
        sys.stdout.flush()
        preds = probs[:, 0] > thresh
        TP = sum(preds & true_labels)
        TN = sum(~preds & ~true_labels)
        FP = sum(preds & ~true_labels)
        FN = sum(~preds & true_labels)
        if TP+FN==0:
            tpr += [1]
        else:
            tpr += [TP/(TP+FN)]
        if TN+FP==0:
            fpr += [1]
        else:
            fpr += [FP/(FP+TN)]
    sys.stdout.write("\rROC: done" + 30*' ' + '\n')
    AUC = abs(round(auc(fpr, tpr), 5))
    
    brier = brier_score_loss(true_labels.astype(int), probs[:, 0])

    fig, ax = plt.subplots()
    for i in np.linspace(0,1,num=11):
        ax.axhline(i, linestyle='--', color='k', alpha=0.4, linewidth=0.5)
        ax.axvline(i, linestyle='--', color='k', alpha=0.4, linewidth=0.5)
    ax.set_xlim(-0.05,1.05)
    ax.set_ylim(-0.05,1.05)
    ax.plot(fpr, tpr, 'r', alpha=0.5)
    ax.plot([0,1], [0,1], 'g:')
    ax.text(0.6, 0.25, 'AUC = ' + str(AUC))
    model_name = args.predict_file[:-9]
    split_lines = model_name.split('/')[1].split('_')
    test_name = model_name.split('/')[2]
    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate',
           title='ROC Curve for ' + ' '.join(split_lines) + ' ' + test_name)
    fig.savefig(model_name[:-6] + test_name + '_roc' + '.png', dpi=500)
    
    eval_file = args.predict_file.replace('_pred.txt', '_eval.txt')
    message = (
        f"Prediction file:  {args.predict_file}\n"
        f"TPR:              {TPR}\n"
        f"FPR:              {FPR}\n"
        f"Accuracy:         {accuracy}\n"
        f"Error rate:       {error_rate}\n"
        f"Precision:        {precision}\n"
        f"F-score:          {F_score}\n"
        f"AUC:              {AUC}\n"
        f"Brier score loss: {brier}\n"
    )
    print(message)
    print("Evaluations stored in", eval_file)

    with open(eval_file, 'w') as f:
        f.write(message)
