"""
Evaluate predictions in a prediction file generated by predict.py
"""
import argparse
import os.path as path
import matplotlib.pyplot as plt
from sklearn.metrics import auc, brier_score_loss, confusion_matrix
import numpy as np
import sys

def evaluate_predictions(predict_file):    
    lines = open(predict_file, "r").readlines()
    pairs = [l.strip().split("\t")[1:] for l in lines]

    binary_labels = np.array(pairs)[:, 0] == "TRUE"
    binary_preds = np.array(pairs)[:, 1] == "TRUE"

    tn, fp, fn, tp = confusion_matrix(binary_labels, binary_preds).ravel()

    tpr = tp / (tp+fn)
    fpr = fp / (tn+fp)
    accuracy = (tp+tn) / (tp+tn+fp+fn)
    error_rate = 1 - accuracy
    precision = tp / (tp+fp) if tp+fp != 0 else "NA"
    f_score = tp / (tp+0.5*(fp+fn))

    probs_file = predict_file.replace("_pred.txt", "_probs.txt")
    with open(probs_file, "r") as f:
        str_probs = [l.rstrip().split() for l in f.readlines()]
        probs = np.array([[float(l[0]), float(l[1])] for l in str_probs])

    tpr_roc = []
    fpr_roc = []
    for thresh in np.linspace(0, 1, num=100):
        sys.stdout.write(
            f"\rROC: Working on threshold {thresh//0.01/100} / 1.00 "
        )
        sys.stdout.flush()

        preds = probs[:, 0] > thresh
        tn_roc, fp_roc, fn_roc, tp_roc = confusion_matrix(
            binary_labels,
            preds
        ).ravel()

        tpr_roc += [tp_roc/(tp_roc+fn_roc)] if tp_roc+fn_roc != 0 else [1]
        fpr_roc += [fp_roc/(fp_roc+tn_roc)] if fp_roc+tn_roc != 0 else [1]

    sys.stdout.write(f"\rROC: done{30*' '}\n")

    auc_score = abs(round(auc(fpr_roc, tpr_roc), 5))
    brier = brier_score_loss(binary_labels.astype(int), probs[:, 0])

    fig, ax = plt.subplots()
    for i in np.linspace(0, 1, num=11):
        ax.axhline(i, linestyle="--", color="k", alpha=0.4, linewidth=0.5)
        ax.axvline(i, linestyle="--", color="k", alpha=0.4, linewidth=0.5)
    ax.set_xlim(-0.05, 1.05)
    ax.set_ylim(-0.05, 1.05)
    ax.plot(fpr_roc, tpr_roc, "r", alpha=0.5)
    ax.plot([0,1], [0,1], "g:")
    ax.text(0.6, 0.25, f"AUC = {auc_score}")
    model_name = predict_file.replace("_pred.txt", "")
    split_lines = model_name.split("/")[1].split("_")
    test_name = model_name.split("/")[2]
    model_test_name = " ".join(split_lines)
    ax.set(
        xlabel="False Positive Rate",
        ylabel="True Positive Rate",
        title=f"ROC Curve for {model_test_name} {test_name}"
    )
    fig.savefig(f"{model_name[:-6]}{test_name}_roc.png", dpi=500)
    
    eval_file = predict_file.replace("_pred.txt", "_eval.txt")
    message = (
        f"Prediction file: {predict_file}\n"
        f"TP:              {tp}\n"
        f"TN:              {tn}\n"
        f"FP:              {fp}\n"
        f"FN:              {fn}\n"
        f"TPR:             {tpr}\n"
        f"FPR:             {fpr}\n"
        f"Accuracy:        {accuracy}\n"
        f"Error rate:      {error_rate}\n"
        f"Precision:       {precision}\n"
        f"F-score:         {f_score}\n"
        f"AUC:             {auc_score}\n"
        f"Brier:           {brier}\n"
    )
    print(message)
    print(f"Evaluations stored in {eval_file}")

    with open(eval_file, "w") as f:
        f.write(message)
