"""
Evaluate predictions in a prediction file generated by predict.py
"""

from sklearn.metrics import auc, brier_score_loss, confusion_matrix
import numpy as np
import sys


def evaluate_predictions(predict_file):
    lines = open(predict_file, "r").readlines()
    pairs = [l.strip().split("\t")[1:] for l in lines]

    binary_labels = np.array(pairs)[:, 0] == "TRUE"
    binary_preds = np.array(pairs)[:, 1] == "TRUE"

    tn, fp, fn, tp = confusion_matrix(binary_labels, binary_preds).ravel()

    tpr = tp / (tp + fn)
    fpr = fp / (tn + fp)
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    error_rate = 1 - accuracy
    precision = tp / (tp + fp) if tp + fp != 0 else "NA"
    f_score = tp / (tp + 0.5 * (fp + fn))

    probs_file = predict_file.replace("_pred.txt", "_probs.txt")
    with open(probs_file, "r") as f:
        str_probs = [l.rstrip().split() for l in f.readlines()]
        probs = np.array([[float(l[0]), float(l[1])] for l in str_probs])

    tpr_roc = []
    fpr_roc = []
    for thresh in np.linspace(0, 1, num=100):
        sys.stdout.write(f"\rROC: Working on threshold {thresh//0.01/100} / 1.00 ")
        sys.stdout.flush()

        preds = probs[:, 0] > thresh
        tn_roc, fp_roc, fn_roc, tp_roc = confusion_matrix(binary_labels, preds).ravel()

        tpr_roc += [tp_roc / (tp_roc + fn_roc)] if tp_roc + fn_roc != 0 else [1]
        fpr_roc += [fp_roc / (fp_roc + tn_roc)] if fp_roc + tn_roc != 0 else [1]

    sys.stdout.write(f"\rROC: done{30*' '}\n")

    auc_score = abs(round(auc(fpr_roc, tpr_roc), 5))
    brier = brier_score_loss(binary_labels.astype(int), probs[:, 0])

    eval_file = predict_file.replace("_pred.txt", "_eval.txt")
    message = (
        f"Prediction file: {predict_file}\n"
        f"TP:              {tp}\n"
        f"TN:              {tn}\n"
        f"FP:              {fp}\n"
        f"FN:              {fn}\n"
        f"TPR:             {tpr}\n"
        f"FPR:             {fpr}\n"
        f"Accuracy:        {accuracy}\n"
        f"Error rate:      {error_rate}\n"
        f"Precision:       {precision}\n"
        f"F-score:         {f_score}\n"
        f"AUC:             {auc_score}\n"
        f"Brier:           {brier}\n"
    )
    print(message)
    print(f"Evaluations stored in {eval_file}")

    with open(eval_file, "w") as f:
        f.write(message)

